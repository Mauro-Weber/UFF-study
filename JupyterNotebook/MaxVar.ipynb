{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cc78e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------+--------------------+-------------------+-------------------+--------------------+--------------------+--------------------+\n",
      "| id|coord_x|coord_y|                  fv|distances_oi_pivot1|distances_oi_pivot2|  lower_oq_1_pivot_1|  lower_oq_1_pivot_2|    lower_bound_oq_1|\n",
      "+---+-------+-------+--------------------+-------------------+-------------------+--------------------+--------------------+--------------------+\n",
      "| 85|0.76609|0.64468|[0.76608997583389...|0.41778435337450587| 0.8028971733507899| 0.20707898361741833| 0.46906233987117324| 0.46906233987117324|\n",
      "| 86|0.26943|0.14308|[0.26943001151084...| 0.3719507735888958| 0.8703225256674223|  0.2529125634030284|  0.5364876921878057|  0.5364876921878057|\n",
      "| 87|0.14337|0.68237|[0.14337000250816...| 0.6465445658220822|0.31772170702902175|0.021681228830157995|0.016113126450594928|0.021681228830157995|\n",
      "| 88|0.50937|0.54759|[0.50937002897262...| 0.3181158510618258| 0.6413638134022074|  0.3067474859300984|  0.3075289799225907|  0.3075289799225907|\n",
      "| 89|0.37869|0.58439|[0.37869000434875...|0.41439892559855857| 0.5239005155502525| 0.21046441139336564| 0.19006568207063584| 0.21046441139336564|\n",
      "| 90|0.48126| 0.9986|[0.48126000165939...| 0.7610101627387232| 0.4441228835862516| 0.13614682574679904| 0.11028805010663495| 0.13614682574679904|\n",
      "| 91|0.75064|0.51948|[0.75063997507095...|0.29599718122448343| 0.8499921905784616| 0.32886615576744077|  0.5161573570988449|  0.5161573570988449|\n",
      "| 92|0.45075|0.24551|[0.45074999332427...|0.17462411512215484|  0.844459047695574|  0.4502392218697694|  0.5106242142159574|  0.5106242142159574|\n",
      "| 93|0.74777|0.67109|[0.74777001142501...| 0.4372569357236911| 0.7753495076587901| 0.18760640126823308|  0.4415146741791734|  0.4415146741791734|\n",
      "| 94|0.13182|0.28656|[0.13181999325752...|0.49471193741877323| 0.7017309915207973| 0.13015139957315097| 0.36789615804118064| 0.36789615804118064|\n",
      "| 95|0.30788|0.98741|[0.30788001418113...| 0.8015846121766239| 0.2704852205193587|  0.1767212751846997| 0.06334961296025798|  0.1767212751846997|\n",
      "| 96| 0.8488|0.78662|[0.84880000352859...| 0.5800741539991952| 0.8345229979485522| 0.04478918299272905|  0.5006881644689356|  0.5006881644689356|\n",
      "| 97|0.53746|0.99852|[0.53746002912521...| 0.7523216630445868| 0.5002850379452707|  0.1274583260526626|   0.166450204465654|   0.166450204465654|\n",
      "| 98|0.15969|0.32973|[0.15968999266624...| 0.4721395951318474| 0.6635433864299096|  0.1527237418600768| 0.32970855295029294| 0.32970855295029294|\n",
      "| 99|0.67736|0.56179|[0.67735999822616...|  0.314787371946192| 0.7655001147536822|  0.3100759650457322|  0.4316652812740655|  0.4316652812740655|\n",
      "|100|0.54705|0.17514|[0.54704999923706...| 0.1092064595769778| 0.9542431834716354|  0.5156568774149464|  0.6204083499920188|  0.6204083499920188|\n",
      "| 49|0.90466|0.82299|[0.90465998649597...| 0.6362682664199504| 0.8816517520191985|0.011404929428026178|  0.5478169185395818|  0.5478169185395818|\n",
      "| 50|0.35363|0.69526|[0.35363000631332...| 0.5204380707180242| 0.4267803169322903| 0.10442526627390003| 0.09294548345267362| 0.10442526627390003|\n",
      "| 51|0.46455|0.33096|[0.46454998850822...|0.17936618604290755| 0.7785617420862937| 0.44549715094901665|   0.444726908606677| 0.44549715094901665|\n",
      "| 52|0.90945|0.71154|[0.90944999456405...|  0.540867199495352| 0.9129548105023487|  0.0839961374965722|   0.579119977022732|   0.579119977022732|\n",
      "+---+-------+-------+--------------------+-------------------+-------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "##\n",
    "## ---------------------------------------------------------------------------\n",
    "##\n",
    "\n",
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import DoubleType, FloatType, StructType, StructField, IntegerType,  StringType\n",
    "\n",
    "## the distance function\n",
    "from scipy.spatial import distance\n",
    "## creates the feature vector\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "## import numpy\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql.functions import max, abs, pow, sum, col\n",
    "\n",
    "from pyspark.sql.functions import greatest\n",
    "\n",
    "import time\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from statistics import mode\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import heapq\n",
    "\n",
    "\n",
    "## start session\n",
    "spark = SparkSession.builder.appName(\"SparkLAESAKnn\").getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##DEFINES A UDF FROM L2DIST THE DATAFRAME FROM A QUERY SET\n",
    "distance_udf = F.udf(lambda x,y: float(distance.euclidean(x, y)), DoubleType())\n",
    "\n",
    "def simpleF(oq):\n",
    "    return F.udf(lambda x: float(distance.euclidean(x, oq)), DoubleType())\n",
    "\n",
    "## defines query object and \n",
    "#oq = [np.random.rand(1)[0],np.random.rand(1)[0]]\n",
    "oq = [(1.0,[0.312,0.792])]\n",
    "oqColumns = [\"idOq\",\"fvOq\"]\n",
    "oqDF = spark.createDataFrame(data=oq, schema = oqColumns)\n",
    "#oqDF.show()\n",
    "\n",
    "\n",
    "## defines neighbrs amount\n",
    "k = 10\n",
    "\n",
    "# define pivots amount\n",
    "pivots = 2\n",
    "\n",
    "nPivots = 2\n",
    "\n",
    "# df\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"coord_x\", FloatType(), True),\n",
    "    StructField(\"coord_y\", FloatType(), True)])\n",
    "\n",
    "df = spark.read.csv(\"/home/weber/Documents/coordDF100.csv\",schema=schema)        \n",
    "\n",
    "\n",
    "size = df.count()\n",
    "##SMALL SANITY CHECK - @PRODUCTION TESTAR LOADING\n",
    "df = df.na.drop()\n",
    "\n",
    "## define the struct for the dimensional feature vector\n",
    "cNames = df.columns\n",
    "cNames.remove(\"id\")\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=cNames,\n",
    "    outputCol=\"fv\")\n",
    "\n",
    "## appends the fv into the dataframe as column\n",
    "df = assembler.transform(df)\n",
    "\n",
    "\n",
    "# creates random pivots\n",
    "pivots_list = [(i+1,[np.random.rand(1)[0],np.random.rand(1)[0]]) for i in range(pivots)]\n",
    "\n",
    "## calculates the distance from oi to pivots\n",
    "for i in range(len(pivots_list)):\n",
    "    df = df.withColumn(f'distances_oi_pivot{i+1}', simpleF(pivots_list[i][1])(F.col('fv')))\n",
    "\n",
    "    \n",
    "##\n",
    "##\n",
    "##\n",
    "lista_columns = []\n",
    "\n",
    "rows_oqDF = oqDF.collect()\n",
    "\n",
    "for oq in rows_oqDF: \n",
    "    ##\n",
    "    ## DISTANCE FROM OQ TO PIVOT\n",
    "    ##\n",
    "    lista_columns = []\n",
    "    for i in range(len(pivots_list)):\n",
    "        df = df.withColumn(f\"lower_oq_{int(oq.idOq)}_pivot_{i+1}\", \\\n",
    "            abs (distance.euclidean(oq.fvOq, pivots_list[i][1]) - \\\n",
    "            F.col(f'distances_oi_pivot{i+1}')))\n",
    "\n",
    "        lista_columns.append(f\"lower_oq_{int(oq.idOq)}_pivot_{i+1}\")\n",
    "\n",
    "    df = df.withColumn(f\"lower_bound_oq_{int(oq.idOq)}\", greatest(*[col_name for col_name in lista_columns]))\n",
    "\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b43654",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Assuming your DataFrame is named `df`\n",
    "#x_col = 'coord_x'\n",
    "#y_col = 'coord_y'\n",
    "\n",
    "# Extract the x and y coordinates from the DataFrame\n",
    "x = df.select(\"coord_x\").rdd.flatMap(lambda x: x).collect()\n",
    "y = df.select(\"coord_y\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Plot the points on a Cartesian plane\n",
    "plt.scatter(x, y)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a5f9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sum_of_squares(df,size):\n",
    "    return df.groupBy(\"features\", \"avg(distance)\").agg(\n",
    "        (sum(pow(col(\"distance\") - col(\"avg(distance)\"), 2))/size).alias(\"variance\")\n",
    "    )\n",
    "\n",
    "def maxVariance(df, nPivots):\n",
    "    # create sample dataset\n",
    "    dataset = df.select(\"id\",\"coord_x\",\"coord_y\")\n",
    "\n",
    "    # create a random sample of the input dataset\n",
    "    sample1 = dataset.sample(False, 0.1, seed=42)\n",
    "    sample2 = dataset.sample(False, 0.1, seed=24)\n",
    "\n",
    "    # convert dataset to vectors\n",
    "    cNames = dataset.columns\n",
    "    cNames.remove(\"id\")\n",
    "    assembler = VectorAssembler(inputCols=cNames,outputCol=\"features\")\n",
    "\n",
    "\n",
    "    #assembler = VectorAssembler(inputCols=dataset.columns, outputCol=\"features\")\n",
    "    dataset_vectors = assembler.transform(dataset)\n",
    "    \n",
    "    dataset_vectors.show()\n",
    "    \n",
    "    sample_vectors1 = assembler.transform(sample1)\n",
    "    sample_vectors2 = assembler.transform(sample2).withColumnRenamed(\"features\",\"features2\")\n",
    "\n",
    "    sample_vectors2.show()\n",
    "    # calculate mean distance between objects in part1 and part2\n",
    "    cross_df = sample_vectors1.crossJoin(sample_vectors2.select(\"features2\"))\n",
    "\n",
    "    cross_df.show()\n",
    "    \n",
    "    distance = cross_df.withColumn(\"distance\", distance_udf(\"features\", \"features2\"))\n",
    "\n",
    "    mean_distance = distance.groupBy(\"features\").agg({\"distance\": \"mean\"})\n",
    "\n",
    "    new_df = mean_distance.join(distance, on=\"features\", how=\"inner\")\n",
    "\n",
    "    variance_df = calculate_sum_of_squares(new_df,(sample_vectors2.count()+1))\n",
    "    rows = variance_df.select(\"features\", \"variance\").collect()\n",
    "    list_var = [(row.features, row.variance) for row in rows]\n",
    "    sorted_lst = sorted(list_var, key=lambda x: x[1], reverse=True)[0:nPivots]\n",
    "    \n",
    "    return(sorted_lst)\n",
    "\n",
    "maxVariance(df, nPivots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcd5190a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxVariance:\n",
    "    \n",
    "    def __init__(self, df, nPivots):\n",
    "        self.df = df\n",
    "        self.nPivots = nPivots\n",
    "        \n",
    "    def distance_udf(self):\n",
    "        return F.udf(lambda x, y: float(distance.euclidean(x, y)), DoubleType())\n",
    "    \n",
    "    def calculate_sum_of_squares(self, df, size):\n",
    "        return df.groupBy(\"features\", \"avg(distance)\").agg(\n",
    "            (sum(pow(col(\"distance\") - col(\"avg(distance)\"), 2))/size).alias(\"variance\")\n",
    "        )\n",
    "    \n",
    "    def find_max_variance(self):\n",
    "        # create sample dataset\n",
    "        dataset = self.df.select(\"id\",\"coord_x\", \"coord_y\")\n",
    "\n",
    "        # create a random sample of the input dataset\n",
    "        sample1 = dataset.sample(False, 0.1, seed=42)\n",
    "        sample2 = dataset.sample(False, 0.1, seed=24)\n",
    "\n",
    "        # convert dataset to vectors\n",
    "        cNames = dataset.columns\n",
    "        cNames.remove(\"id\")\n",
    "        assembler = VectorAssembler(inputCols=cNames,outputCol=\"features\")\n",
    "        \n",
    "        \n",
    "        #assembler = VectorAssembler(inputCols=dataset.columns, outputCol=\"features\")\n",
    "        dataset_vectors = assembler.transform(dataset).select(\"features\")\n",
    "        sample_vectors1 = assembler.transform(sample1).select(\"features\")\n",
    "        sample_vectors2 = assembler.transform(sample2).select(\"features\").withColumnRenamed(\"features\",\"features2\")\n",
    "\n",
    "        # calculate mean distance between objects in part1 and part2\n",
    "        cross_df = sample_vectors1.crossJoin(sample_vectors2)\n",
    "        distance = cross_df.withColumn(\"distance\", self.distance_udf()(\"features\", \"features2\"))\n",
    "        mean_distance = distance.groupBy(\"features\").agg({\"distance\": \"mean\"})\n",
    "        \n",
    "        # calculate variance between objects in part1 and part2\n",
    "        new_df = mean_distance.join(distance, on=\"features\", how=\"inner\")\n",
    "        variance_df = self.calculate_sum_of_squares(new_df, (sample_vectors2.count() + 1))\n",
    "        \n",
    "        # save the variances and sort it\n",
    "        rows = variance_df.select(\"features\", \"variance\").collect()\n",
    "        list_var = [(row.features, row.variance) for row in rows]\n",
    "        sorted_lst = sorted(list_var, key=lambda x: x[1], reverse=True)[0:self.nPivots]\n",
    "        \n",
    "        return sorted_lst\n",
    "    \n",
    "    def find_max_variance_fullset(self):\n",
    "        # create sample dataset\n",
    "        dataset = self.df.select(\"id\",\"coord_x\", \"coord_y\")\n",
    "\n",
    "        # create a random sample of the input dataset\n",
    "        sample1 = dataset\n",
    "        sample2 = dataset\n",
    "\n",
    "        # convert dataset to vectors\n",
    "        cNames = dataset.columns\n",
    "        cNames.remove(\"id\")\n",
    "        assembler = VectorAssembler(inputCols=cNames,outputCol=\"features\")\n",
    "        \n",
    "        \n",
    "        #assembler = VectorAssembler(inputCols=dataset.columns, outputCol=\"features\")\n",
    "        dataset_vectors = assembler.transform(dataset).select(\"features\")\n",
    "        sample_vectors1 = assembler.transform(sample1).select(\"features\")\n",
    "        sample_vectors2 = assembler.transform(sample2).select(\"features\").withColumnRenamed(\"features\",\"features2\")\n",
    "\n",
    "        # calculate mean distance between objects in part1 and part2\n",
    "        cross_df = sample_vectors1.crossJoin(sample_vectors2)\n",
    "        distance = cross_df.withColumn(\"distance\", self.distance_udf()(\"features\", \"features2\"))\n",
    "        mean_distance = distance.groupBy(\"features\").agg({\"distance\": \"mean\"})\n",
    "        \n",
    "        # calculate variance between objects in part1 and part2\n",
    "        new_df = mean_distance.join(distance, on=\"features\", how=\"inner\")\n",
    "        variance_df = self.calculate_sum_of_squares(new_df, (sample_vectors2.count() + 1))\n",
    "        \n",
    "        # save the variances and sort it\n",
    "        rows = variance_df.select(\"features\", \"variance\").collect()\n",
    "        list_var = [(row.features, row.variance) for row in rows]\n",
    "        sorted_lst = sorted(list_var, key=lambda x: x[1], reverse=True)[0:self.nPivots]\n",
    "        \n",
    "        return sorted_lst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "206e8adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(DenseVector([0.8887, 0.3965]), 0.09519428005514337), (DenseVector([0.1539, 0.2099]), 0.08109210623909925)]\n",
      "[(DenseVector([0.1561, 0.0042]), 0.08716387386690382), (DenseVector([0.1582, 0.0275]), 0.08606545175876348)]\n"
     ]
    }
   ],
   "source": [
    "max_variance_obj = MaxVariance(df, nPivots)\n",
    "max_variance = max_variance_obj.find_max_variance()\n",
    "max_variance2 = max_variance_obj.find_max_variance_fullset()\n",
    "print(max_variance)\n",
    "print(max_variance2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e689463d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
